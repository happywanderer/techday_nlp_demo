{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welome to Tech Day NLP Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\593379\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from os import path\n",
    "from string import punctuation\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    df = pd.read_csv('./data/file.csv')\n",
    "    \n",
    "    # Create a temporary list of sources starting from the first instance of sources \n",
    "    # and returning unique values in the 1st column (the column containing source names)\n",
    "    # This also pulls in extra instances of 'Source' in the dataframe\n",
    "    temp_sources = list(df.iloc[((df[df[df.columns[1]] == 'Source'].index[0])+1):,1].unique())\n",
    "    \n",
    "    # Create source list and use a for loop to fill the source list\n",
    "    # using the temporary source list but excluding 'Source'\n",
    "    source_list = []\n",
    "    for i in temp_sources:\n",
    "        if i != 'Source':\n",
    "            source_list.append(i.lower())\n",
    "    \n",
    "    # Sanity Check\n",
    "    # print(source_list)\n",
    "    \n",
    "    # Determine the index values in the data frame where 'Source' \n",
    "    # is found in column 1. This is used later to slice the dataframe into\n",
    "    # sub dataframes\n",
    "    source_breaks = list(df[df[df.columns[1]] == 'Source'].index)\n",
    "    \n",
    "    # sanity check\n",
    "    # print(source_breaks)\n",
    "    \n",
    "    # print(len(source_list), len(source_breaks))\n",
    "    \n",
    "    # create some empty holder lists and dictionaries; 'd' is going to be a dictionary of dataframes\n",
    "    # The other temp lists and dictionary are used in creating these sub dataframes and associated column names\n",
    "    d = {}\n",
    "    bad_cols = []\n",
    "    good_cols = []\n",
    "    temp_dict = {}\n",
    "    \n",
    "    # Create a for loop to iterate through source_list to construct sub dataframes\n",
    "    for i, vals in enumerate(source_list):\n",
    "        \n",
    "        # Create a temp var 'j' for indexing/slicing the dataframe with iloc\n",
    "        j = i+1\n",
    "        \n",
    "        # Create an if/else to test if the value of 'j' exceeds the length of the source_list\n",
    "        # If it does not, create some sub dataframes using the value of 'j' as the 'stop' point \n",
    "        # for iloc slice.\n",
    "        #\n",
    "        # If it does, use the length of the main dataframe as the 'stop' point for the iloc slice\n",
    "        if j < len(source_list):\n",
    "            d['{0}'.format(vals)] = df.iloc[source_breaks[i]+1: source_breaks[j]]\n",
    "            \n",
    "            # Rename the columns by creating a dictionary from the bad column names and\n",
    "            # the actual column names found at a specific location in the main dataframe\n",
    "            bad_cols = list(d['{0}'.format(vals)].columns)\n",
    "            good_cols = list(df.iloc[source_breaks[i]])\n",
    "            temp_dict = dict(zip(bad_cols, good_cols))\n",
    "            \n",
    "            d['{0}'.format(vals)].rename(columns=temp_dict, inplace=True)\n",
    "            d['{0}'.format(vals)].reset_index(inplace=True, drop=True)\n",
    "            \n",
    "        else:\n",
    "            d['{0}'.format(vals)] = df.iloc[source_breaks[i]+1:]\n",
    "            \n",
    "            # Rename the columns by creating a dictionary from the bad column names and\n",
    "            # the actual column names found at a specific location in the main dataframe\n",
    "            bad_cols = list(d['{0}'.format(vals)].columns)\n",
    "            good_cols = list(df.iloc[source_breaks[i]])\n",
    "            temp_dict = dict(zip(bad_cols, good_cols))\n",
    "            d['{0}'.format(vals)].rename(columns=temp_dict, inplace=True)\n",
    "            d['{0}'.format(vals)].reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # This for loop formats date-time columns to appropriate datatype.\n",
    "    # This is by far the most time-instensive process of this function\n",
    "    # Time savings could be realized if ONLY dates are needed and not time & date.\n",
    "    #\n",
    "    # I am also using this for loop to trim unneeded rows in the INSTAGRAM\n",
    "    # and YOUTUBE dataframes.\n",
    "    #\n",
    "    # By using the lambda function and dictionary generator, I was able to achieve a\n",
    "    # 4 second increase in processing speed.\n",
    "    for key in d.keys():\n",
    "        \n",
    "        # Because INSTAGRAM and YOUTUBE have different column structures than all others,\n",
    "        # set up an if/else structure to process these separately\n",
    "        if key == 'instagram':\n",
    "           \n",
    "            # Dictionary generator to create a dates dictionary with the unique date string\n",
    "            # from the original dataframe and associating the to_datetime formatted date to \n",
    "            # said string\n",
    "            dates = {date:pd.to_datetime(date) for date in d[key]['Date'].unique()}\n",
    "            \n",
    "            # use a lambda function to apply the new dates to the dataframe\n",
    "            d[key]['Date'] = d[key]['Date'].apply(lambda v: dates[v])\n",
    "            \n",
    "            # Now use an apply function to apply to_datetime to all entries in the 'Time' column\n",
    "            d[key]['Time'] = d[key]['Time'].apply(pd.to_datetime, errors='ignore')\n",
    "            \n",
    "            #d[key][['Date', 'Time']] = d[key][['Date', 'Time']].apply(pd.to_datetime, errors='ignore')\n",
    "            \n",
    "            # Trim the dataframe to only relevant rows\n",
    "            d[key] = d[key].iloc[:, 2:24]\n",
    "        elif key == 'youtube':\n",
    "            dates = {date:pd.to_datetime(date) for date in d[key]['Date'].unique()}\n",
    "            d[key]['Date'] = d[key]['Date'].apply(lambda v: dates[v])\n",
    "            d[key]['Time'] = d[key]['Time'].apply(pd.to_datetime, errors='ignore')\n",
    "            #d[key][['Date', 'Time']] = d[key][['Date', 'Time']].apply(pd.to_datetime, errors='ignore')\n",
    "            d[key] = d[key].iloc[:, 2:17]\n",
    "        else:\n",
    "            dates = {date:pd.to_datetime(date) for date in d[key]['Date(ET)'].unique()}\n",
    "            d[key]['Date(ET)'] = d[key]['Date(ET)'].apply(lambda v: dates[v])\n",
    "            d[key][['Time(ET)', 'LocalTime']] = d[key][['Time(ET)', 'LocalTime']].apply(pd.to_datetime, errors='ignore')\n",
    "            d[key] = d[key].iloc[:, 2:]\n",
    "            \n",
    "    clean_cols(d)\n",
    "    \n",
    "    \n",
    "    for key in d.keys():\n",
    "        d['{0}'.format(key)]['Cleaned_Text'] = d['{0}'.format(key)]['Cleaned_Text'].apply(cleanup)\n",
    "    \n",
    "    files_out(d)\n",
    "    \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_out(d):\n",
    "    # Create an output directory for the new dataframes\n",
    "    # if the directory already exists, just pass\n",
    "    try:\n",
    "        os.makedirs('./data/clean_dfs/')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    # Set up a for loop to iterate through all dataframes in the dictionary\n",
    "    # If an output file does NOT already exist for that dataframe, \n",
    "    # create a new output file\n",
    "    #\n",
    "    # If the output file DOES exist, open that file in 'append' mode\n",
    "    # and append the new data to the end without header information.\n",
    "    #\n",
    "    # Keeping the pandas index in place, so when data is read in for manipulation later,\n",
    "    # set index_col = 0\n",
    "    #\n",
    "    # Also, index will have to be reset upon initial read in later as this append method\n",
    "    # does not reset the index.  I am doing this in the interest of time savings.\n",
    "    \n",
    "    for key in d.keys():\n",
    "        if not path.exists('./data/clean_dfs/{0}.csv'.format(key)):\n",
    "            print('file {0}.csv does not exist. Creating new file'.format(key))\n",
    "            d[key].to_csv('./data/clean_dfs/{0}.csv'.format(key), index=False)\n",
    "        else:\n",
    "            print('file {0}.csv does exist. Appending new data to existing file'.format(key))\n",
    "            df_temp = pd.read_csv('./data/clean_dfs/{0}.csv'.format(key))\n",
    "            df_temp = df_temp.append(d[key], ignore_index=True)\n",
    "            df_temp.to_csv('./data/clean_dfs/{0}.csv'.format(key), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cols(df):\n",
    "    for key in df.keys():\n",
    "        df['{0}'.format(key)]['Cleaned_Text'] = df['{0}'.format(key)]['Contents']\n",
    "\n",
    "    for key in df.keys():\n",
    "        if key != 'instagram':\n",
    "            if df['{0}'.format(key)]['Summary'] is not np.NaN:\n",
    "                df['{0}'.format(key)]['Cleaned_Text'].fillna(df['{0}'.format(key)]['Summary'], inplace=True)\n",
    "            if df['{0}'.format(key)]['Title'] is not np.NaN:\n",
    "                df['{0}'.format(key)]['Cleaned_Text'].fillna(df['{0}'.format(key)]['Title'], inplace=True)\n",
    "            if df['{0}'.format(key)]['Snippet'] is not np.NaN: \n",
    "                df['{0}'.format(key)]['Cleaned_Text'].fillna(df['{0}'.format(key)]['Snippet'], inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    \n",
    "    #rusw_df = pd.read_csv('./Jim/rusw.csv')\n",
    "    #stopwords = list(rusw_df.columns[0:1001])\n",
    "    #stopwords_string = ''\n",
    "    #for\n",
    "        #stopwords_string = stopwords_string + i +' '\n",
    "        \n",
    "    ru_stopwords = stopwords.words('russian')\n",
    "    rustopwords = ' '.join(ru_stopwords[:])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #cstart = time.time()\n",
    "    # insert a space between hashtags\n",
    "    \n",
    "    text = re.sub(\"\\#\",\" \", text)\n",
    "    \n",
    "    # remove urls \n",
    "    \n",
    "    text = re.sub(\"(http|https)\\:\\/\\/.*\\s?\",\"\",text)\n",
    "    \n",
    "    # remove HTML tags\n",
    "    \n",
    "    text = re.sub(\"\\<.*\\>\",\"\",text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', punctuation))\n",
    "    #cend = time.time()\n",
    "    #print(text)\n",
    "    #print(\"cleanup time:\",cend - cstart)\n",
    "   \n",
    "    text = re.sub(rustopwords, '', text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokens = [token for token in text.lower().split() if token not in rustopwords\\\n",
    "            and token != \" \"]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    \n",
    "    #return text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\593379\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3214: DtypeWarning:\n",
      "\n",
      "Columns (25,26,27,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n",
      "C:\\Users\\593379\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4025: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\593379\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:108: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\593379\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3391: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\593379\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\593379\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file news.csv does not exist. Creating new file\n",
      "file forums.csv does not exist. Creating new file\n",
      "file tumblr.csv does not exist. Creating new file\n",
      "file twitter.csv does not exist. Creating new file\n",
      "file instagram.csv does not exist. Creating new file\n",
      "101.20800828933716\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = read_data()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "twitterdf = pd.read_csv('./data/clean_dfs/forums.csv', parse_dates=['Date(ET)', 'LocalTime', 'Time(ET)'], date_parser=dateparse)\n",
    "\n",
    "#twitterdf = pd.read_csv('./data/clean_dfs/twitter.csv', parse_dates = True)\n",
    "\n",
    "#df.groupby(pd.Grouper(key='Date(ET)', freq='M'))['Cleaned_Text'].sum()\n",
    "\n",
    "#twitterdf.set_index('Date(ET)').groupby('Date(ET)')['Cleaned_Text'].resample(\"M\").sum()\n",
    "print(twitterdf.dtypes)\n",
    "display(twitterdf.head())\n",
    "\n",
    "\n",
    "dfthing = pd.DataFrame(twitterdf.groupby(pd.Grouper(key='Date(ET)', freq='W'))['Contents'].count())\n",
    "\n",
    "dfthing.reset_index(drop=False, inplace=True)\n",
    "dfthing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_list = list(dfthing.Contents)\n",
    "date_list = list(dfthing['Date(ET)'])\n",
    "#print(count_list, date_list)\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(x='Date(ET)', y='Contents', data=dfthing, color='red', lw=4)\n",
    "plt.xlabel('Years', fontsize=22)\n",
    "plt.ylabel('Post Count', fontsize=22)\n",
    "plt.title('Trending', fontsize=22, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gapminder = plotly.data.dfthing()\n",
    "fig = plotly.line(gapminder, x=\"Date(ET)\", y=\"Contents\", title='Life expectancy in Canada')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
