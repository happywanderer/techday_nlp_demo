{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welome to Tech Day NLP Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#    The first function\n",
    "#\n",
    "#    This function reads in the data from the csv file\n",
    "#    It then processes the data into separate dataframes\n",
    "#    These dataframes are contained within a dictionary\n",
    "#    The function returns the dictionary of dataframes\n",
    "#    for further processing\n",
    "###\n",
    "\n",
    "def read_data():\n",
    "    df = pd.read_csv('./data/file.csv')\n",
    "    \n",
    "    # Initialize an empty dictionary to become the dictionary of dataframes\n",
    "    d = {}    \n",
    "\n",
    "    # rename the columns from crap into good stuff based on actual column names from csv file\n",
    "    columns_good = list(df.iloc[(df[df[df.columns[1]] == 'Source'].index[0])])\n",
    "    columns_bad = list(df.columns)\n",
    "    cols_dict = dict(zip(columns_bad, columns_good))\n",
    "    \n",
    "    df.rename(columns=cols_dict, inplace=True)\n",
    "\n",
    "    # Sanity check printing\n",
    "    # print(len(columns_good))\n",
    "    # print(len(columns_bad))\n",
    "    # cols_dict\n",
    "    \n",
    "    # Find all unique sources dropping garbage at top of CSV file\n",
    "    source_list = list(df.iloc[((df[df[df.columns[1]] == 'Source'].index[0])+1):,1].unique())\n",
    "    \n",
    "    # display(df.iloc[((df[df[df.columns[1]] == 'Source'].index[0])+1):,1])\n",
    "    \n",
    "    # Sanity check\n",
    "    # print(source_list)    \n",
    "\n",
    "    # create a list of the columsn that contain date/time information (used in the for loop below)\n",
    "    dt_cols = ['Date(ET)', 'Time(ET)', 'LocalTime']\n",
    "    \n",
    "    # Certain Sources have different column structures. \n",
    "    # Create columnn lists for each source with the correct column headings\n",
    "    youtube_cols = \"No,Source,Link,Date,Time,Author,Author Profile,Category,Title,Description,Views,Comments,Likes,Dislikes,Favourites,Duration (seconds),Unique ID\".split(sep=',')\n",
    "    instagram_cols = \"No,Source,Link,Date,Time,Author ID,Author Name,Language,Location,Contents,HashTags,Likes,Comments,Attachments,Brand Images,Object Images,Food Images,Scene Images,Selfie,Sentiment,Themes,Classifications,Entities,Unique ID\".split(sep=',')\n",
    "    allother_cols = \"No,Source,Host,Link,Date(ET),Time(ET),LocalTime,Category,Author ID,Author Name,Author URL,Authority,Followers,Following,Age,Gender,Language,Country,Province/State,City,Location,Sentiment,Themes,Classifications,Entities,Alexa Rank,Alexa Reach,Title,Snippet,Contents,Summary,Bio,Unique ID,Post Source\".split(sep=',')\n",
    "    \n",
    "    # Create a for loop to build a dictionary of dataframes\n",
    "    for i in source_list:\n",
    "        d['{0}'.format(i)] = df[df.iloc[:,1] == i]\n",
    "        d['{0}'.format(i)].reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        # Fix the datetime dtype issue.\n",
    "        # This applies the to_datetime function to the three identified rows that contain date/time data\n",
    "        # \n",
    "        # IMPORTANT\n",
    "        # Because all sources were merged into ONE dataframe, some info in the date/time columns was NOT\n",
    "        # date/time info. For this reason, we use 'errors='ignore''. This keeps the original data that was\n",
    "        # NOT date/time data intact for when we split the data out into individual dataframes below\n",
    "        for col in dt_cols:\n",
    "            d['{0}'.format(i)].loc[:, col] = pd.to_datetime(d['{0}'.format(i)].loc[:, col], errors='ignore')\n",
    "        \n",
    "        temp_dict = {}\n",
    "        \n",
    "        if i == 'INSTAGRAM':\n",
    "            d['{0}'.format(i)] = d['{0}'.format(i)].iloc[:,0:24]\n",
    "            temp_dict = dict(zip(d['{0}'.format(i)].columns, instagram_cols))\n",
    "            d['{0}'.format(i)].rename(columns=temp_dict, inplace=True)\n",
    "        elif i == 'YOUTUBE':\n",
    "            d['{0}'.format(i)] = d['{0}'.format(i)].iloc[:,0:17]\n",
    "            temp_dict = dict(zip(d['{0}'.format(i)].columns, youtube_cols))\n",
    "            d['{0}'.format(i)].rename(columns=temp_dict, inplace=True)\n",
    "        else:\n",
    "            temp_dict = dict(zip(d['{0}'.format(i)].columns, allother_cols))\n",
    "            d['{0}'.format(i)].rename(columns=temp_dict, inplace=True)\n",
    "        \n",
    "    # Create a for loop to drop columns that are completely NaN in EACH dataframe\n",
    "    # Also drop any rows that are completely NaN in EACH dataframe\n",
    "    #for j in source_list:\n",
    "    #    print(d[j].shape)    # Sanity Check\n",
    "    #    d[j].dropna(axis=1, how='all', inplace=True)\n",
    "    #    print(d[j].shape)    # Sanity Check\n",
    "    #    d[j].dropna(axis=0, how='all', inplace=True)\n",
    "    #    print(d[j].shape)    # Sanity Check\n",
    "    #    d[j].reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    # Return the dictionary of dataframes \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.606229543685913\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = read_data()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data2():\n",
    "    df = pd.read_csv('./data/file.csv')\n",
    "    \n",
    "    # Create a temporary list of sources starting from the first instance of sources \n",
    "    # and returning unique values in the 1st column (the column containing source names)\n",
    "    # This also pulls in extra instances of 'Source' in the dataframe\n",
    "    temp_sources = list(df.iloc[((df[df[df.columns[1]] == 'Source'].index[0])+1):,1].unique())\n",
    "    \n",
    "    # Create source list and use a for loop to fill the source list\n",
    "    # using the temporary source list but excluding 'Source'\n",
    "    source_list = []\n",
    "    for i in temp_sources:\n",
    "        if i != 'Source':\n",
    "            source_list.append(i)\n",
    "    \n",
    "    # Sanity Check\n",
    "    # print(source_list)\n",
    "    \n",
    "    # Determine the index values in the data frame where 'Source' \n",
    "    # is found in column 1. This is used later to slice the dataframe into\n",
    "    # sub dataframes\n",
    "    source_breaks = list(df[df[df.columns[1]] == 'Source'].index)\n",
    "    \n",
    "    # sanity check\n",
    "    # print(source_breaks)\n",
    "    \n",
    "    # print(len(source_list), len(source_breaks))\n",
    "    \n",
    "    # create some empty holder lists and dictionaries; 'd' is going to be a dictionary of dataframes\n",
    "    # The other temp lists and dictionary are used in creating these sub dataframes and associated column names\n",
    "    d = {}\n",
    "    bad_cols = []\n",
    "    good_cols = []\n",
    "    temp_dict = {}\n",
    "    \n",
    "    # Create a for loop to iterate through source_list to construct sub dataframes\n",
    "    for i, vals in enumerate(source_list):\n",
    "        \n",
    "        # Create a temp var 'j' for indexing/slicing the dataframe with iloc\n",
    "        j = i+1\n",
    "        \n",
    "        # Create an if/else to test if the value of 'j' exceeds the length of the source_list\n",
    "        # If it does not, create some sub dataframes using the value of 'j' as the 'stop' point \n",
    "        # for iloc slice.\n",
    "        #\n",
    "        # If it does, use the length of the main dataframe as the 'stop' point for the iloc slice\n",
    "        if j < len(source_list):\n",
    "            d['{0}'.format(vals)] = df.iloc[source_breaks[i]+1: source_breaks[j]]\n",
    "            \n",
    "            # Rename the columns by creating a dictionary from the bad column names and\n",
    "            # the actual column names found at a specific location in the main dataframe\n",
    "            bad_cols = list(d['{0}'.format(vals)].columns)\n",
    "            good_cols = list(df.iloc[source_breaks[i]])\n",
    "            temp_dict = dict(zip(bad_cols, good_cols))\n",
    "            \n",
    "            d['{0}'.format(vals)].rename(columns=temp_dict, inplace=True)\n",
    "            d['{0}'.format(vals)].reset_index(inplace=True, drop=True)\n",
    "            \n",
    "        else:\n",
    "            d['{0}'.format(vals)] = df.iloc[source_breaks[i]+1:]\n",
    "            \n",
    "            # Rename the columns by creating a dictionary from the bad column names and\n",
    "            # the actual column names found at a specific location in the main dataframe\n",
    "            bad_cols = list(d['{0}'.format(vals)].columns)\n",
    "            good_cols = list(df.iloc[source_breaks[i]])\n",
    "            temp_dict = dict(zip(bad_cols, good_cols))\n",
    "            d['{0}'.format(vals)].rename(columns=temp_dict, inplace=True)\n",
    "            d['{0}'.format(vals)].reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    for key in d.keys():\n",
    "        if key == 'INSTAGRAM' or key == 'YOUTUBE':\n",
    "            d[key][['Date', 'Time']] = d[key][['Date', 'Time']].apply(pd.to_datetime, errors='ignore')\n",
    "        else:\n",
    "            d[key][['Date(ET)', 'Time(ET)', 'LocalTime']] = d[key][['Date(ET)', 'Time(ET)', 'LocalTime']].apply(pd.to_datetime, errors='ignore')\n",
    "    \n",
    "    \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.53195238113403\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_crazy = read_data2()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['NEWS', 'FORUMS', 'TUMBLR', 'TWITTER', 'INSTAGRAM'])\n"
     ]
    }
   ],
   "source": [
    "print(df_crazy.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Source</th>\n",
       "      <th>Host</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date(ET)</th>\n",
       "      <th>Time(ET)</th>\n",
       "      <th>LocalTime</th>\n",
       "      <th>Category</th>\n",
       "      <th>Author ID</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Author URL</th>\n",
       "      <th>Authority</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Following</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Language</th>\n",
       "      <th>Country</th>\n",
       "      <th>Province/State</th>\n",
       "      <th>City</th>\n",
       "      <th>Location</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Themes</th>\n",
       "      <th>Classifications</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Alexa Rank</th>\n",
       "      <th>Alexa Reach</th>\n",
       "      <th>Title</th>\n",
       "      <th>Snippet</th>\n",
       "      <th>Contents</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Bio</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Post Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>http://sakhalinmedia.ru/</td>\n",
       "      <td>https://sakhalinmedia.ru/news/824534/</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>2019-07-02 23:15:17</td>\n",
       "      <td>2019-06-20 23:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>ru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42565</td>\n",
       "      <td>42119</td>\n",
       "      <td>Афиша выходного дня: Сабантуй в лучах \"Утра Ро...</td>\n",
       "      <td>ИА SakhalinMedia рассказывает, куда пойти и чт...</td>\n",
       "      <td>ИА SakhalinMedia рассказывает, куда пойти и чт...</td>\n",
       "      <td>ИА SakhalinMedia рассказывает, куда пойти и чт...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>http://mirtesen.sputnik.ru/</td>\n",
       "      <td>http://mirtesen.sputnik.ru/blog/43460930378/Af...</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>2019-07-02 23:11:37</td>\n",
       "      <td>2019-06-20 23:11:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>ru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57760</td>\n",
       "      <td>29160</td>\n",
       "      <td>Афиша выходного дня: Сабантуй в лучах \"Утра Ро...</td>\n",
       "      <td>Спутник 21 июня, 5:40 Прекрасная погода и уйма...</td>\n",
       "      <td>Спутник 21 июня, 5:40 Прекрасная погода и уйма...</td>\n",
       "      <td>Спутник 21 июня, 5:40 Прекрасная погода и уйма...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>https://news.myseldon.com/ru/</td>\n",
       "      <td>https://news.myseldon.com/ru/news/index/212361...</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>2019-07-02 20:13:27</td>\n",
       "      <td>2019-06-20 20:13:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>ru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79537</td>\n",
       "      <td>18000</td>\n",
       "      <td>Афиша выходного дня: Сабантуй в лучах \"Утра Ро...</td>\n",
       "      <td>ИА SakhalinMedia рассказывает, куда пойти и чт...</td>\n",
       "      <td>ИА SakhalinMedia рассказывает, куда пойти и чт...</td>\n",
       "      <td>ИА SakhalinMedia рассказывает, куда пойти и чт...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>http://www.trud.ru/</td>\n",
       "      <td>http://www.trud.ru/article/21-06-2019/1377277_...</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>2019-07-02 18:25:43</td>\n",
       "      <td>2019-06-20 18:25:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Анна Владимирова все статьи автора</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>ru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>business;finance;</td>\n",
       "      <td>Org.business;</td>\n",
       "      <td>Citigroup;</td>\n",
       "      <td>61151</td>\n",
       "      <td>28800</td>\n",
       "      <td>Ольга Кабо: Бессмысленно воспринимать меня сек...</td>\n",
       "      <td>«Сама-то я человек XXI века, мне комфортно жит...</td>\n",
       "      <td>«Сама-то я человек XXI века, мне комфортно жит...</td>\n",
       "      <td>«Сама-то я человек XXI века, мне комфортно жит...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>https://www.bragazeta.ru/</td>\n",
       "      <td>https://www.bragazeta.ru/world/2847395/</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>2019-07-02 16:00:28</td>\n",
       "      <td>2019-06-20 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>ru</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ru</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32305</td>\n",
       "      <td>59220</td>\n",
       "      <td>От «психотерапии» - к агрессивному НЛП-убежден...</td>\n",
       "      <td>Естественно, разговор с населением неизбежно д...</td>\n",
       "      <td>Естественно, разговор с населением неизбежно д...</td>\n",
       "      <td>Естественно, разговор с населением неизбежно д...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  No Source                           Host  \\\n",
       "0  1   NEWS       http://sakhalinmedia.ru/   \n",
       "1  2   NEWS    http://mirtesen.sputnik.ru/   \n",
       "2  3   NEWS  https://news.myseldon.com/ru/   \n",
       "3  4   NEWS            http://www.trud.ru/   \n",
       "4  5   NEWS      https://www.bragazeta.ru/   \n",
       "\n",
       "                                                Link   Date(ET)  \\\n",
       "0              https://sakhalinmedia.ru/news/824534/ 2019-06-20   \n",
       "1  http://mirtesen.sputnik.ru/blog/43460930378/Af... 2019-06-20   \n",
       "2  https://news.myseldon.com/ru/news/index/212361... 2019-06-20   \n",
       "3  http://www.trud.ru/article/21-06-2019/1377277_... 2019-06-20   \n",
       "4            https://www.bragazeta.ru/world/2847395/ 2019-06-20   \n",
       "\n",
       "             Time(ET)           LocalTime Category Author ID  \\\n",
       "0 2019-07-02 23:15:17 2019-06-20 23:15:00      NaN       NaN   \n",
       "1 2019-07-02 23:11:37 2019-06-20 23:11:00      NaN       NaN   \n",
       "2 2019-07-02 20:13:27 2019-06-20 20:13:00      NaN       NaN   \n",
       "3 2019-07-02 18:25:43 2019-06-20 18:25:00      NaN       NaN   \n",
       "4 2019-07-02 16:00:28 2019-06-20 16:00:00      NaN       NaN   \n",
       "\n",
       "                          Author Name Author URL Authority Followers  \\\n",
       "0                                 NaN        NaN         0       NaN   \n",
       "1                                 NaN        NaN         0       NaN   \n",
       "2                                 NaN        NaN         0       NaN   \n",
       "3  Анна Владимирова все статьи автора        NaN         2       NaN   \n",
       "4                                 NaN        NaN         0       NaN   \n",
       "\n",
       "  Following  Age Gender Language Country Province/State City Location  \\\n",
       "0       NaN  NaN    NaN       ru      ru            NaN  NaN       ru   \n",
       "1       NaN  NaN    NaN       ru      ru            NaN  NaN       ru   \n",
       "2       NaN  NaN    NaN       ru      ru            NaN  NaN       ru   \n",
       "3       NaN  NaN    NaN       ru      ru            NaN  NaN       ru   \n",
       "4       NaN  NaN    NaN       ru      ru            NaN  NaN       ru   \n",
       "\n",
       "  Sentiment             Themes Classifications    Entities Alexa Rank  \\\n",
       "0   NEUTRAL                NaN             NaN         NaN      42565   \n",
       "1   NEUTRAL                NaN             NaN         NaN      57760   \n",
       "2   NEUTRAL                NaN             NaN         NaN      79537   \n",
       "3   NEUTRAL  business;finance;   Org.business;  Citigroup;      61151   \n",
       "4   NEUTRAL                NaN             NaN         NaN      32305   \n",
       "\n",
       "  Alexa Reach                                              Title  \\\n",
       "0       42119  Афиша выходного дня: Сабантуй в лучах \"Утра Ро...   \n",
       "1       29160  Афиша выходного дня: Сабантуй в лучах \"Утра Ро...   \n",
       "2       18000  Афиша выходного дня: Сабантуй в лучах \"Утра Ро...   \n",
       "3       28800  Ольга Кабо: Бессмысленно воспринимать меня сек...   \n",
       "4       59220  От «психотерапии» - к агрессивному НЛП-убежден...   \n",
       "\n",
       "                                             Snippet  \\\n",
       "0  ИА SakhalinMedia рассказывает, куда пойти и чт...   \n",
       "1  Спутник 21 июня, 5:40 Прекрасная погода и уйма...   \n",
       "2  ИА SakhalinMedia рассказывает, куда пойти и чт...   \n",
       "3  «Сама-то я человек XXI века, мне комфортно жит...   \n",
       "4  Естественно, разговор с населением неизбежно д...   \n",
       "\n",
       "                                            Contents  \\\n",
       "0  ИА SakhalinMedia рассказывает, куда пойти и чт...   \n",
       "1  Спутник 21 июня, 5:40 Прекрасная погода и уйма...   \n",
       "2  ИА SakhalinMedia рассказывает, куда пойти и чт...   \n",
       "3  «Сама-то я человек XXI века, мне комфортно жит...   \n",
       "4  Естественно, разговор с населением неизбежно д...   \n",
       "\n",
       "                                             Summary  Bio Unique ID  \\\n",
       "0  ИА SakhalinMedia рассказывает, куда пойти и чт...  NaN       NaN   \n",
       "1  Спутник 21 июня, 5:40 Прекрасная погода и уйма...  NaN       NaN   \n",
       "2  ИА SakhalinMedia рассказывает, куда пойти и чт...  NaN       NaN   \n",
       "3  «Сама-то я человек XXI века, мне комфортно жит...  NaN       NaN   \n",
       "4  Естественно, разговор с населением неизбежно д...  NaN       NaN   \n",
       "\n",
       "  Post Source  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crazy['NEWS'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 17)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_youtube = pd.read_csv('./data/youtube.csv')\n",
    "df_youtube.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
