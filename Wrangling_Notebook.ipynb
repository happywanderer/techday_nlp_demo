{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welome to Tech Day NLP Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#    The first function\n",
    "#\n",
    "#    This function reads in the data from the csv file\n",
    "#    It then processes the data into separate dataframes\n",
    "#    These dataframes are contained within a dictionary\n",
    "#    The function returns the dictionary of dataframes\n",
    "#    for further processing\n",
    "###\n",
    "\n",
    "def read_data():\n",
    "    df = pd.read_csv('./data/file.csv')\n",
    "    \n",
    "    # Initialize an empty dictionary to become the dictionary of dataframes\n",
    "    d = {}    \n",
    "\n",
    "    # rename the columns from crap into good stuff based on actual column names from csv file\n",
    "    columns_good = list(df.iloc[(df[df[df.columns[1]] == 'Source'].index[0])])\n",
    "    columns_bad = list(df.columns)\n",
    "    cols_dict = dict(zip(columns_bad, columns_good))\n",
    "    \n",
    "    df.rename(columns=cols_dict, inplace=True)\n",
    "\n",
    "    # Sanity check printing\n",
    "    # print(len(columns_good))\n",
    "    # print(len(columns_bad))\n",
    "    # cols_dict\n",
    "    \n",
    "    # Find all unique sources dropping garbage at top of CSV file\n",
    "    source_list = list(df.iloc[((df[df[df.columns[1]] == 'Source'].index[0])+1):,1].unique())\n",
    "    \n",
    "    # display(df.iloc[((df[df[df.columns[1]] == 'Source'].index[0])+1):,1])\n",
    "    \n",
    "    # Sanity check\n",
    "    # print(source_list)    \n",
    "\n",
    "    # create a list of the columsn that contain date/time information (used in the for loop below)\n",
    "    dt_cols = ['Date(ET)', 'Time(ET)', 'LocalTime']\n",
    "    \n",
    "    # Certain Sources have different column structures. \n",
    "    # Create columnn lists for each source with the correct column headings\n",
    "    youtube_cols = \"No,Source,Link,Date,Time,Author,Author Profile,Category,Title,Description,Views,Comments,Likes,Dislikes,Favourites,Duration (seconds),Unique ID\".split(sep=',')\n",
    "    instagram_cols = \"No,Source,Link,Date,Time,Author ID,Author Name,Language,Location,Contents,HashTags,Likes,Comments,Attachments,Brand Images,Object Images,Food Images,Scene Images,Selfie,Sentiment,Themes,Classifications,Entities,Unique ID\".split(sep=',')\n",
    "    allother_cols = \"No,Source,Host,Link,Date(ET),Time(ET),LocalTime,Category,Author ID,Author Name,Author URL,Authority,Followers,Following,Age,Gender,Language,Country,Province/State,City,Location,Sentiment,Themes,Classifications,Entities,Alexa Rank,Alexa Reach,Title,Snippet,Contents,Summary,Bio,Unique ID,Post Source\".split(sep=',')\n",
    "    \n",
    "    # Create a for loop to build a dictionary of dataframes\n",
    "    for i in source_list:\n",
    "        d['{0}'.format(i)] = df[df.iloc[:,1] == i]\n",
    "        d['{0}'.format(i)].reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        # Fix the datetime dtype issue.\n",
    "        # This applies the to_datetime function to the three identified rows that contain date/time data\n",
    "        # \n",
    "        # IMPORTANT\n",
    "        # Because all sources were merged into ONE dataframe, some info in the date/time columns was NOT\n",
    "        # date/time info. For this reason, we use 'errors='ignore''. This keeps the original data that was\n",
    "        # NOT date/time data intact for when we split the data out into individual dataframes below\n",
    "        for col in dt_cols:\n",
    "            d['{0}'.format(i)].loc[:, col] = pd.to_datetime(d['{0}'.format(i)].loc[:, col], errors='ignore')\n",
    "        \n",
    "        temp_dict = {}\n",
    "        \n",
    "        if i == 'INSTAGRAM':\n",
    "            d['{0}'.format(i)] = d['{0}'.format(i)].iloc[:,0:24]\n",
    "            temp_dict = dict(zip(d['{0}'.format(i)].columns, instagram_cols))\n",
    "            d['{0}'.format(i)].rename(columns=temp_dict, inplace=True)\n",
    "        elif i == 'YOUTUBE':\n",
    "            d['{0}'.format(i)] = d['{0}'.format(i)].iloc[:,0:17]\n",
    "            temp_dict = dict(zip(d['{0}'.format(i)].columns, youtube_cols))\n",
    "            d['{0}'.format(i)].rename(columns=temp_dict, inplace=True)\n",
    "        else:\n",
    "            temp_dict = dict(zip(d['{0}'.format(i)].columns, allother_cols))\n",
    "            d['{0}'.format(i)].rename(columns=temp_dict, inplace=True)\n",
    "        \n",
    "    # Create a for loop to drop columns that are completely NaN in EACH dataframe\n",
    "    # Also drop any rows that are completely NaN in EACH dataframe\n",
    "    #for j in source_list:\n",
    "    #    print(d[j].shape)    # Sanity Check\n",
    "    #    d[j].dropna(axis=1, how='all', inplace=True)\n",
    "    #    print(d[j].shape)    # Sanity Check\n",
    "    #    d[j].dropna(axis=0, how='all', inplace=True)\n",
    "    #    print(d[j].shape)    # Sanity Check\n",
    "    #    d[j].reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    # Return the dictionary of dataframes \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.606229543685913\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = read_data()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data2():\n",
    "    df = pd.read_csv('./data/file.csv')\n",
    "    \n",
    "    # Create a temporary list of sources starting from the first instance of sources \n",
    "    # and returning unique values in the 1st column (the column containing source names)\n",
    "    # This also pulls in extra instances of 'Source' in the dataframe\n",
    "    temp_sources = list(df.iloc[((df[df[df.columns[1]] == 'Source'].index[0])+1):,1].unique())\n",
    "    \n",
    "    # Create source list and use a for loop to fill the source list\n",
    "    # using the temporary source list but excluding 'Source'\n",
    "    source_list = []\n",
    "    for i in temp_sources:\n",
    "        if i != 'Source':\n",
    "            source_list.append(i)\n",
    "    \n",
    "    # Sanity Check\n",
    "    # print(source_list)\n",
    "    \n",
    "    # Determine the index values in the data frame where 'Source' \n",
    "    # is found in column 1. This is used later to slice the dataframe into\n",
    "    # sub dataframes\n",
    "    source_breaks = list(df[df[df.columns[1]] == 'Source'].index)\n",
    "    \n",
    "    # sanity check\n",
    "    # print(source_breaks)\n",
    "    \n",
    "    # print(len(source_list), len(source_breaks))\n",
    "    \n",
    "    # create some empty holder lists and dictionaries; 'd' is going to be a dictionary of dataframes\n",
    "    # The other temp lists and dictionary are used in creating these sub dataframes and associated column names\n",
    "    d = {}\n",
    "    bad_cols = []\n",
    "    good_cols = []\n",
    "    temp_dict = {}\n",
    "    \n",
    "    # Create a for loop to iterate through source_list to construct sub dataframes\n",
    "    for i, vals in enumerate(source_list):\n",
    "        \n",
    "        # Create a temp var 'j' for indexing/slicing the dataframe with iloc\n",
    "        j = i+1\n",
    "        \n",
    "        # Create an if/else to test if the value of 'j' exceeds the length of the source_list\n",
    "        # If it does not, create some sub dataframes using the value of 'j' as the 'stop' point \n",
    "        # for iloc slice.\n",
    "        #\n",
    "        # If it does, use the length of the main dataframe as the 'stop' point for the iloc slice\n",
    "        if j < len(source_list):\n",
    "            d['{0}'.format(vals)] = df.iloc[source_breaks[i]+1: source_breaks[j]]\n",
    "            \n",
    "            # Rename the columns by creating a dictionary from the bad column names and\n",
    "            # the actual column names found at a specific location in the main dataframe\n",
    "            bad_cols = list(d['{0}'.format(vals)].columns)\n",
    "            good_cols = list(df.iloc[source_breaks[i]])\n",
    "            temp_dict = dict(zip(bad_cols, good_cols))\n",
    "            \n",
    "            d['{0}'.format(vals)].rename(columns=temp_dict, inplace=True)\n",
    "            \n",
    "        else:\n",
    "            d['{0}'.format(vals)] = df.iloc[source_breaks[i]+1:]\n",
    "            \n",
    "            # Rename the columns by creating a dictionary from the bad column names and\n",
    "            # the actual column names found at a specific location in the main dataframe\n",
    "            bad_cols = list(d['{0}'.format(vals)].columns)\n",
    "            good_cols = list(df.iloc[source_breaks[i]])\n",
    "            temp_dict = dict(zip(bad_cols, good_cols))\n",
    "            d['{0}'.format(vals)].rename(columns=temp_dict, inplace=True)\n",
    "    \n",
    "    for key in d.keys():\n",
    "        if key == 'INSTAGRAM' or key == 'YOUTUBE':\n",
    "            d[key][['Date', 'Time']] = d[key][['Date', 'Time']].apply(pd.to_datetime, errors='ignore')\n",
    "        else:\n",
    "            d[key][['Date(ET)', 'Time(ET)', 'LocalTime']] = d[key][['Date(ET)', 'Time(ET)', 'LocalTime']].apply(pd.to_datetime, errors='ignore')\n",
    "    \n",
    "    \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.510170221328735\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_crazy = read_data2()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['NEWS', 'FORUMS', 'TUMBLR', 'TWITTER', 'INSTAGRAM'])\n"
     ]
    }
   ],
   "source": [
    "print(df_crazy.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
